{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1f59780-7547-4828-966b-7405e01e1268",
   "metadata": {},
   "source": [
    "# python 파일 ipynb로 통합\n",
    "- 파일 통합\n",
    "- 통합하고 주석달아서 필기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7981aa-11cf-4a9a-80cd-871a2a86dcb7",
   "metadata": {},
   "source": [
    "## 0. 라이브러리 , 패키지 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05fa313c-85b2-4edb-9d33-610517275c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from copy import deepcopy\n",
    "import argparse # 명령행 패키지\n",
    "\n",
    "# 연산\n",
    "import numpy as np\n",
    "\n",
    "# 파이토치\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim  # optimizer 패키지"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ee3cc1-1a17-406e-8f7e-cb02041a23ea",
   "metadata": {},
   "source": [
    "## 1. model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06a52a0c-c099-4175-956c-9922bc61c846",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # FCL 구성\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, 500),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm1d(500),\n",
    "            nn.Linear(500, 400),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm1d(400),\n",
    "            nn.Linear(400, 300),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm1d(300),\n",
    "            nn.Linear(300, 200),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm1d(200),\n",
    "            nn.Linear(200, 100),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm1d(100),\n",
    "            nn.Linear(100, 50),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm1d(50),\n",
    "            nn.Linear(50, output_size),\n",
    "            nn.Softmax(dim=-1),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # |x| = (batch_size, input_size) = (batch_size, 784(28x28))\n",
    "        y = self.layers(x)\n",
    "        # |y| = (batch_size, output_size(=10)) = y_hat\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f9b7a5-15fd-4bfb-a45c-6c972a4ce1d7",
   "metadata": {},
   "source": [
    "## 2. trainer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "895c5c38-bd0b-4347-ac74-b033dcc6608d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "\n",
    "    def __init__(self, model, optimizer, crit):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.crit = crit\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "    def _train(self, x, y, config):\n",
    "        self.model.train()  # 모델을 train 모드로 전환. 빼먹지말고 항상 명심할것 \n",
    "\n",
    "        # Shuffle before begin / 매 에포크마다 x,y 데이터를 섞어줘야 한다.\n",
    "        # x, y 페어를 이뤄서 섞어줘야한다.\n",
    "        # x개만큼(mnist면 60000개) 랜덤 인덱스를 만들어준다.\n",
    "        indices = torch.randperm(x.size(0), device=x.device)\n",
    "        # 6만개 랜덤 인덱스로 index_select로 원본데이터를 섞어준다.\n",
    "        # 그리고 split으로 배치 개수만큼으로 쪼개준다. \n",
    "        # 배치 사이즈가 100이면 x에는 랜덤으로 섞인 60개의 배치묶음이 생긴다.\n",
    "        x = torch.index_select(x, dim=0, index=indices).split(config.batch_size, dim=0)\n",
    "        y = torch.index_select(y, dim=0, index=indices).split(config.batch_size, dim=0)\n",
    "\n",
    "        total_loss = 0\n",
    "\n",
    "        for i, (x_i, y_i) in enumerate(zip(x, y)):\n",
    "            y_hat_i = self.model(x_i)\n",
    "            # Crossentropy Loss\n",
    "            loss_i = self.crit(y_hat_i, y_i.squeeze()) # 혹시모를 차원을 맞추기위한 스퀴즈\n",
    "\n",
    "            # Initialize the gradients of the model\n",
    "            self.optimizer.zero_grad() # 기존 기울기 초기화\n",
    "            loss_i.backward() # 역전파\n",
    "\n",
    "            self.optimizer.step() # 최적화 진행\n",
    "\n",
    "            if config.verbose >= 2:\n",
    "                print(\n",
    "                    f'Train Iteration{i+1}/{len(x)} : loss = {float(loss_i):.4f}')\n",
    "\n",
    "            # Don't forget to detach to prevent memory leak.\n",
    "            # loss_i는 텐서형이다. 그리고 지금껏 쓰인 계산그래프가 저장되어있다.\n",
    "            # float을 안하고 그냥 하면 total_loss에 모든 iteration의 computational graph가 물려있게된다.\n",
    "            # 결론적으로 이것들이 다 메모리를 잡고있어서 코스트가 증가하는 꼴(메모리 누수)\n",
    "            # 그렇기에 파이썬의 float형으로 바꿔 없애주자.\n",
    "            total_loss += float(loss_i)\n",
    "\n",
    "        return total_loss / len(x)\n",
    "\n",
    "    def _validate(self, x, y, config):\n",
    "        # Turn evaluation mode on.\n",
    "        self.model.eval() # 꼭 evaluation 모드로 변경해야한다.\n",
    "\n",
    "        # Turn on the no_grad mode to make more efficiently\n",
    "        with torch.no_grad(): # 검증엔 경사하강이 필요없다.\n",
    "            # Shuffle before begin. 검증인 셔플안해도될수도 있지만 그냥 했다.\n",
    "            indices = torch.randperm(x.size(0), device=x.device)\n",
    "            x = torch.index_select(x, dim=0, index=indices).split(config.batch_size, dim=0)\n",
    "            y = torch.index_select(y, dim=0, index=indices).split(config.batch_size, dim=0)\n",
    "\n",
    "            total_loss = 0\n",
    "\n",
    "            for i, (x_i, y_i) in enumerate(zip(x, y)):\n",
    "                y_hat_i = self.model(x_i)\n",
    "                loss_i = self.crit(y_hat_i, y_i.squeeze())\n",
    "\n",
    "                if config.verbose >= 2:\n",
    "                    print(\n",
    "                        f'Valid Iteration{i+1}/{len(x)} : loss = {float(loss_i):.4f}')\n",
    "\n",
    "                total_loss += float(loss_i)\n",
    "\n",
    "            return total_loss / len(x)\n",
    "\n",
    "    def train(self, train_data, valid_data, config):\n",
    "        lowest_loss = np.inf\n",
    "        best_model = None\n",
    "\n",
    "        for epoch_index in range(config.n_epochs):\n",
    "            train_loss = self._train(train_data[0], train_data[i], config)\n",
    "            valid_loss = self._validate(valid_data[0], valid_data[1], config)\n",
    "\n",
    "            # You must use deep copy to take a snapshot of current best weights.\n",
    "            # deepcopy를 안해주면 for돌때마다 best_model이 계속바뀐다. (메모리주소참조)\n",
    "            if valid_loss <= lowest_loss:\n",
    "                lowest_loss = valid_loss\n",
    "                best_model = deepcopy(self.model.state_dict())\n",
    "\n",
    "            print(f'Epoch{epoch_index + 1}/{config.n_epochs} : train_loss = {train_loss:.4d}, valid_loss = {valid_loss:.4d}, lowest_loss = {lowest_loss:.4d}')\n",
    "\n",
    "        # Restore to best model\n",
    "        self.model.load_state_dict(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae177fa-db04-4a1f-a6f7-62ab0c56ee7e",
   "metadata": {},
   "source": [
    "## 3. utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00957a4e-3887-407c-96d7-d160a0bc5aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(is_train=True, flatten=True):\n",
    "    from torchvision import datasets, transforms\n",
    "\n",
    "    dataset = datasets.MNIST(\n",
    "        '../data', train=is_train, download=True,\n",
    "        transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "        ]),\n",
    "    )\n",
    "\n",
    "    x = dataset.data.float() / 255.\n",
    "    y = dataset.targets\n",
    "\n",
    "    if flatten:\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d488b2-9d26-48a2-8821-2cccebfc219e",
   "metadata": {},
   "source": [
    "## 4. train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab90eab9-2d5d-444c-809d-fddca028e779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLI 환경 인자 설정\n",
    "def define_argparser():\n",
    "    p = argparse.ArgumentParser()\n",
    "    \n",
    "    # 저장될 모델, weight파일 이름\n",
    "    p.add_argument('--model_fn', required=True)\n",
    "    # gpu_id\n",
    "    p.add_argument('--gpu_id', type=int, default=0 if torch.cuda.is_available() else -1)\n",
    "    \n",
    "    # train, validation set split ratio\n",
    "    p.add_argument('--train_ratio', type=float, default=.8)\n",
    "\n",
    "    p.add_argument('--batch_size', type=int, default=64)\n",
    "    p.add_argument('--n_epochs', type=int, default=20)\n",
    "    p.add_argument('--verbose', type=int, default=2)\n",
    "\n",
    "    config = p.parse_args()\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "def main(config):\n",
    "    # Set device based on user defined configuration.\n",
    "    device = torch.device('cpu') if config.gpu_id < 0 else torch.device('cuda:%d' % config.gpu_id)\n",
    "\n",
    "    x, y = load_mnist(is_train=True)\n",
    "    # Reshape tensor to chunk of 1-d vectors.\n",
    "    x = x.view(x.size(0), -1)\n",
    "\n",
    "    train_cnt = int(x.size(0) * config.train_ratio)\n",
    "    valid_cnt = x.size(0) - train_cnt\n",
    "\n",
    "    # Shuffle dataset to split into train/valid set.\n",
    "    indices = torch.randperm(x.size(0))\n",
    "    x = torch.index_select(\n",
    "        x,\n",
    "        dim=0,\n",
    "        index=indices\n",
    "    ).to(device).split([train_cnt, valid_cnt], dim=0)\n",
    "    y = torch.index_select(\n",
    "        y,\n",
    "        dim=0,\n",
    "        index=indices\n",
    "    ).to(device).split([train_cnt, valid_cnt], dim=0)\n",
    "\n",
    "    print(\"Train:\", x[0].shape, y[0].shape)\n",
    "    print(\"Valid:\", x[1].shape, y[1].shape)\n",
    "\n",
    "    model = ImageClassifier(28**2, 10).to(device)\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    crit = nn.CrossEntropyLoss() # nn.NLLLoss()\n",
    "\n",
    "    trainer = Trainer(model, optimizer, crit)\n",
    "\n",
    "    trainer.train((x[0], y[0]), (x[1], y[1]), config)\n",
    "\n",
    "    # Save best model weights.\n",
    "    torch.save({\n",
    "        'model': trainer.model.state_dict(),\n",
    "        'config': config,\n",
    "    }, config.model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f8e87b-e9fa-4094-bd99-a1a65eb38063",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    config = define_argparser()\n",
    "    main(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
