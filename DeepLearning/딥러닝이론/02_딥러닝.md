# 딥러닝 이론



## 퍼셉트론 Review

<img src="C:\Users\jay\Desktop\code\ML\딥러닝기본이론\2.png" alt="2" style="zoom:50%;" align="Left"/>

퍼셉트론에 대해 좀 더 자세히 설명하면 ($ bias$를 나타내는 1과 $w_0$을 포함)

- $ inputs = [x_0, x_1, x_2, \cdots, x_{n-1}, x_n]^T $

- $weights = [w_0, w_1, w_2, \cdots, w_{n_1}, w_n]^T $
- $ Weighted \ Sum =  x^T\cdot w = \sum_{i=0}^{n} x_i\cdot w_i$ 

이 가중합을 활성화 함수(activation function)을 지나 조건을 만족하면 1, 아니면 0을 출력하게 된다.

처음에 쓰이던 함수는 step function이다.

- $step\ function =  \begin{cases} 
  0 && if & x\cdot w \le 0 \\
  1 && if & x\cdot w > 0 \end{cases}$  

단순히 임계점을 넘으면 1, 안넘으면 0을 출력하기에 비선형성을 주기에는 부족한 활성화함수라 다음으론 sigmoid 함수가 나오게 되었다.

- $ sigmoid = \frac{1}{1+e^{-x}}$

<img src="C:\Users\jay\Desktop\code\ML\딥러닝기본이론\5.jpg" alt="5" style="zoom:60%;" align="Left"/>



## 퍼셉트론 학습

퍼셉트론을 학습한다라는 건 무엇일까?

$ F(bias + weighted \ sum) = \hat{y} \simeq y$ 를 만족시키는 최적의 weights를 찾는 것이다.

($F$는 활성화함수고 $\hat{y}$는 예측값, $y$는 정답이다.)

초기 weights로 부터 가중합과 활성화함수를 통과시켜 나온 예측값을 정답과 비교해서 오차를 구하고 오차를 줄일 수 있는 방향으로 weights값을 업데이트 시키는 과정이 퍼셉트론의 학습이다.













자료

퍼셉트론 이미지 출처
https://towardsdatascience.com/what-the-hell-is-perceptron-626217814f53

시그모이드 http://ronny.rest/blog/post_2017_08_10_sigmoid/