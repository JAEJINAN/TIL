# 퍼셉트론

1957년 프랭크 로젠블라트가 고안한 선형 분류기
현대의 딥러닝 이론의 기초가 됨

다수의 입력 신호를 받아 하나의 출력 신호를 내보낸다.



## 논리 게이트

### AND 게이트

둘다 참일때만 1, 나머진 0

### OR 게이트

둘중 하나라도 참이면 1, 다 거짓일때만 0

### XOR

둘 중 하나만 참일 때 1, 둘다 참 또는 거짓일 때 0



## 다층 퍼셉트론(multi-layer perceptron)

XOR문제를 풀기위해 고안됨.

입력층과 출력층 사이에 은닉층을 구성함



# 딥러닝

여러 층을 가진 인공 신경망을 사용해 학습을 수행하는 방법



## 딥러닝의 이점

### 특성 추출

- Feature Extraction

  데이터 내에서 의미있는 값을 추출 하는 작업을 feature extraction이라고 한다. (예를 들어 특징을 가지는 데이터를 벡터화시키는 것)

  기존의 머신러닝 기법들은 garbage in, garbage out이라고 쓰레기 값으면 학습결과도 쓰레기가 된다. 이렇기에 모델에서 맞는 학습 데이터를 만드는데 엄청난 시간을 썼다. 도메인 지식도 중요하다. (아는게 있어야 어떤게 중요한 피처인지 구분하니까)

  딥러닝에서는 이러한 특성 추출과정이 알고리즘 속으로 들어갔다. (층이 깊어지면서 파라미터가 많아졌기때문에 가능한 일)
  그렇기에 특성추출을 따라 사람이 해줄 필요는 없다. (물론 데이터 전처리가 필요없다는게 아니다. 전처리는 필요하다.)

### 빅데이터의 효율적 활용

- 딥러닝 학습을 위해서는 데이터양이 방대해야한다. 데이터가 적으면 딥러닝보단 기존 머신러닝을 적용해야한다.
    (제대로된 학습 효과를 기대하기 어려움)

  



## 딥러닝 구성 요소

- 층(layer)
  - 입력층(input layer)
    - 데이터를 받아들이는 층
  - 은닉층(hidden layer)
    - 모든 입력 노드부터 입력 값을 받아 가중합을 구하고, 활성함수에 적용해 출력층에 전달하는 층
  - 출력층(output layer)
    - 신경망의 최종 결과값이 포함된 층
- 가중치(weight)
  - 노드와 노드 간 연결 강도
- 바이어스(bias)
  - 가중합에 더해 주는 상수로, 하나의 뉴런에서 활성화 함수를 거쳐 최종적으로 출력되는 값을 조절하는 역할 함
- 가중합(weighted sum), 전달 함수
  - 가중치와 신호의 곱을 합한 것
- 함수
  - 활성함수(activation function)
    - 신호를 입력 받아 이를 적절히 처리하여 출력해 주는 함수
  - 손실 함수(loss function)
    - 가중치 학습을 위해 출력 함수의 결과와 실제 값 간의 오차를 측정하는 함수



## 노드의 구성

노드는 `전달함수` + `활성화 함수`로 구성된다.

- 전달함수(transfer function) 

  - 가중합과 바이어스의 합
- $ \sum_i w_ix_i + b$

- 활성화 함수

  - 전달함수로부터 받은 값을 출력할 때 거치는 함수

  - 비선형성을 더해준다.

  - Sigmoid

    - $f(x) = \frac{1}{1+e^{-1}}$
    - 0 에서 1사이 비선형 형태로 변형해 준다.
    - 로지스틱 회귀와 같은 분류 문제에서 확률적으로 표현에 쓰임
    - 층이 깊어지면 기울기 소실 문제(vanishing gradient problem)이 발생

  - HyperBolic Tangent

    - -1 에서 1 사이의 비선형 형태로 변형
    - 층이 깊어질수록 기울기 소실이 발생(시그모이드보단 덜함)

  - ReLU (Rectified Linear Unit)

    - 최근 제일 많이 쓰이는 활성 함수
    - 음수는 0, 양수는 그 양수를 따르는(선형) 비선형 함수
    - 경사 하강법에 영향을 주지 않고 학습 속도가 빠르다. (하이퍼볼릭 탄젠드 대비 6배 빠르다 한다.)
    - 기울기 소실 문제가 발생하지 않는다.
    - 일반적으로 은닉층에 사용됨
    - 음수를 입력받으면 0을 출력하는데 이때 학습 능력이 다소 감소하기도 함

  - Leaky ReLU

    - 음수 입력시 0이 아닌 0.001 같은 매우 작은 수를 반환
    - 입력 값이 수렴하는 구간이 제거되어 렐루 함수를 사용할 때 생기는 문제를 해결할 수 있다.

  - Softmax Function

    - $ y_k = \frac{exp(a_k)}{\sum_{i=1}^n exp(a_i)}$

    - 출력을 0~1 사이의 값으로 출력(확률)

    - 출력 노드에 주로 쓰이는 활성 함수

    - 다중 분류에서 쓰임(총 출력의 합이 1)

      ```python
      class Net(torch.nn.Module):
          def __init__(self, n_feature, n_hidden, n_output):
              super(Net, self).__init__()
              self.hidden = torch.nn.Linear(n_feature, n_hidden)
              self.relu = torch.nn.ReLu(inplace=True)
              self.out = torch.nn.Linear(n_hidden, n_output)
              self.softmax = torch.nn.Softmax(dim=n_output)
      	def forward(self, x):
              x = self.hidden(x)
              x = self.relu(x)
              x = self.out(x)
              x = self.softmax(x)
              return x
      ```

      



## 손실함수

경사 하강법은 학습률($\eta$, learning rate)과 손실 함수의 순간 기울기를 이용하여 가중치를 업데이트 하는 방법(역전파)

미분의 기울기를 이용하여 오차를 비교하고 최소화 하는 방향으로 가중치를 조절하는 학습법

여기서 오차를 구하는 방법이 바로 **손실함수**

손실(오차)는 0에 가까울수록 좋은 값이다.

### 평균 제곱 오차(MSE, Mean Square Error)

실제 값과 예측 값의 차이를 제곱하여 평균을 낸 것

주로 회귀에서 손실 함수로 쓰인다.

$$ MSE = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y_i})^2 $$

```python
import torch

loss_fn = torch.nn.MSELoss(reduction='sum')
y_pred = model(x)
loss = loss_fn(y_pred, y)
```



### 크로스 엔트로피 오차(CEE)

분류 문제에서 원-핫 인코딩을 했을 때만 사용할 수 있는 계산법

경사 하강법 과정에서 학습이 지역 최소점에서 멈출 수 있는 것을 방지하고자 로그를 적용했다.

$$ CrossEntropy = -\sum_{i=1}^n y_i log\hat{y}_i $$



```python
loss = nn.CrossEntropyLoss()
input = torch.randn(5, 6, requires_grad=True)
target = torch.empty(3, dtype=torch.long).random_(5)
output = loss(input, target)
output.backward()
```





## 딥러닝의 학습

### 순전파(feedforward)

모든 뉴런이 이전 층의 뉴런에서 받은 정보를 변환해가며 다음 층으로 전달 해 나가는 방식

최종적으로 출력값을 기반으로 예측값을 도출해낸다.

예측값과 정답(label)과의 손실 오차를 구한다.



### 역전파(backpropagation)

손실 오차를 최소화(0으로)하기 위해 역순으로 가중치의 미분값(순간기울기)을 이용해 가중치를 업데이트 해나간다.



순전파와 역전파 한 사이클을 돌면 이것을 1 에포크 (epoch)라고 한다.

이러한 epoch를 계속 반복하면서 지속적으로 모델을 학습 시켜 나간다.





## 딥러닝의 문제점과 해결 방안

### 과적합(over fitting)

훈련 데이터를 과하게 학습시켜 모델이 훈련 데이터에 강한 모습을 보이는 형태

다른 데이터가 들어왔을 때 예측 성능이 떨어진다. (검증 데이터의 오차가 증가할 수도 있다.)

#### 해결 방안 : 드롭아웃(dropout)

신경망 모델이 과적합 되는 것을 피하기 위한 방법으로 학습과정 중 임의의 일부 노드들의 가중치를 0으로 해 학습에서 제외시켜버리는 기법

```python
class DropoutModel(torch.nn.module):
	def __init__(self):
        super(DropoutModel, self).__init__()
        self.layer1 = torch.nn.Linear(784, 1200)
        self.dropout1 = torch.nn.Dropout(0.5)
        self.layer2 = torch.nn.Linear(1200, 1200)
        self.dropout2 = torch.nn.Dropout(0.5)
        self.layer3 = torch.nn.Linear(1200, 10)
        
	def forward(self, x):
        x = F.relu(self.layer1(x))
        x = self.dropout1(x)
        x = F.relu(self.layer2(x))
        x = self.dropout2(x)
        return self.layer3(x)
```



### 기울기 소실 문제

주로 은닉층이 많은 신경망에서 주로 발생

시그모이드와 하이퍼볼릭 탄젠트를 활성함수로 사용시 발생할 수 있다.

기울기가 1보다 작으니 곱하면 곱해질 수록 점점 기울기가 작아져 0에 가까워져 더이상 가중치 업데이트가 안되는 현상

#### 해결방안 : 렐루 활성 함수를 쓰자.





## 옵티마이저

### 배치 경사 하강법(BGD)

- Batch Gradient Descent
- 전체 데이터셋에 대한 오차를 구한 후 기울기를 한번만 계산해 파라미터를 업데이트
- 1스탭(epoch)에서 모든 훈련 데이터를 사용하므로 학습이 느리다.



### 확률적 경사 하강법(SGB)

- Stochastic Gradient Descent
- 배치 경사 하강법의 느린속도를 보완한 경사하강법
- 임의의 데이터를 선택해 기울기를 계산한 후 모델파라미터를 업데이트한다.
- 파라미터 변경 폭이 불안정하고, 때로는 배치 경사하강법보다 정확도가 낮을 수 있지만 속도가 빠르다.



### 미니배치 경사 하강법

- 전체 데이터셋을 미니 배치로 여러개로 나눈다.
- 미니 배치 별로 기울기를 구한 후 평균 기울기를 이용해 모델을 업데이트 한다.
- 배치 경사하강법보다 빠르며, SGD보다 안정적이다.

```python
class CustomDataset(Dataset):
    def __init__(self):
        self.x_data = [[1,2,3], [4,5,6], [7,8,9]]
        self.y_data = [[12], [18], [11]]
		def __len__(self):
            return len(self.x_data)
        def __getitem__(self, idx):
            x = torch.FloatTensor(self.x_data[idx])
            y = torch.FloatTensor(self.y_data[idx])
            return x, y

dataset = CustomDataset()
dataloader = DataLoader(dataset, batch_size=2, suffle=True)
```

### Adagrad

### Adadelta

### RMSProp

### Momentum

### Nesterov Accelerated Gradient, NAG

### Adam







